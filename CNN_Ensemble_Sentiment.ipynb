{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import gensim.models as genmod\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "import keras\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.pooling import MaxPooling1D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.models import Model\n",
    "from keras.layers import Flatten\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def polishData(df):\n",
    "    \n",
    "    data = []\n",
    "\n",
    "    for i in xrange(0, df.shape[0]):\n",
    "        no_links = re.sub(\"https?:\\/\\/.*[\\r\\n]*\", \" \", df.iloc[:,0][i], flags=re.MULTILINE)\n",
    "\n",
    "        letters_only = re.sub(\"[^a-zA-Z]\", \" \", no_links)\n",
    "\n",
    "        lower_case = letters_only.lower()\n",
    "        words = lower_case.split()\n",
    "    \n",
    "        words = [w for w in words if not w in stopwords.words(\"english\")]\n",
    "    \n",
    "        data.append(words)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def labelsOneHot(df):\n",
    "    sentiOneHot = pd.get_dummies(df.iloc[:,1])\n",
    "\n",
    "    labels = np.empty((sentiOneHot.shape), dtype = int)\n",
    "\n",
    "    for i in xrange(0,sentiOneHot.shape[1]):\n",
    "        numbers = np.array(sentiOneHot.iloc[:,i])\n",
    "        labels[:,i] = numbers\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def polishDataSet(df):\n",
    "    data = polishData(df)\n",
    "    labels = labelsOneHot(df)\n",
    "    \n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vocabBuilder (data, unknown = True, min_no_of_words = 1):\n",
    "    tokens = []\n",
    "    \n",
    "    for i in xrange(0, len(data)):\n",
    "        for j in xrange(0,len(data[i])):\n",
    "            tokens.append(data[i][j])\n",
    "            \n",
    "    freqdist = FreqDist(tokens)\n",
    "    \n",
    "    vocab = []\n",
    "    \n",
    "    for key in freqdist:\n",
    "        if freqdist[key] >= min_no_of_words:\n",
    "            vocab.append(key)\n",
    "    \n",
    "    if unknown:\n",
    "        vocab.append('UNKNOWN')\n",
    "    \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fit_unknown_token(data, vocab):\n",
    "    \n",
    "    data_unknown = data\n",
    "    \n",
    "    for i in xrange(0, len(data)):\n",
    "        for j in xrange(0, len(data[i])):\n",
    "            if data[i][j] not in vocab:\n",
    "                data_unknown[i][j] = 'UNKNOWN'\n",
    "                \n",
    "    return data_unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word2vec(data, window, min_count, size, iterations):\n",
    "    model = genmod.Word2Vec(train_data, window = window, min_count = min_count, \n",
    "                            size = size, iter = iterations)\n",
    "    return model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def word_embedding_matrix_builder(word_vectors_model, size, vocab):\n",
    "        \n",
    "    embeddingsMatrix = np.zeros((len(vocab), size))\n",
    "\n",
    "    for i in xrange(0, len(vocab)):\n",
    "        if vocab[i] in word_vectors_model.vocab:\n",
    "            embeddingsMatrix[i] = word_vectors_model[vocab[i]]\n",
    "\n",
    "    return embeddingsMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_to_index(data, vocab):\n",
    "    \n",
    "    data_word_to_index = []\n",
    "    \n",
    "    for i in xrange(0, len(data)):\n",
    "        wordToIndex = []\n",
    "\n",
    "        for j in xrange(0, len(data[i])):            \n",
    "            l = vocab.index(data[i][j])\n",
    "            wordToIndex.append(l)\n",
    "        \n",
    "        data_word_to_index.append(wordToIndex)\n",
    "\n",
    "    return data_word_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makeModel(train_data, train_labels, vocab_length, pretrained = False, wordEmbeddingsLocal = None, \n",
    "              wordEmbeddingsGlove = None, wordEmbeddingsGoogle = None, trainable = False, size = 300,\n",
    "              hidden_layer = 128, activation = 'relu', optimizer = 'adam', loss = 'categorical_crossentropy'):\n",
    "    \n",
    "    main_input = Input(shape = (train_data.shape[1],), dtype = 'float64', name = 'main_input')\n",
    "    word_embeddings_random = Embedding(len(vocab), size, input_length = train_data.shape[1])(main_input)\n",
    "    word_embeddings_pretrained_word2vec_local = Embedding(len(vocab), wordEmbeddingsLocal.shape[1], \n",
    "                                                          input_length = train_data.shape[1],\n",
    "                                                          weights = [wordEmbeddingsLocal], \n",
    "                                                          trainable = trainable) (main_input)\n",
    "    word_embeddings_pretrained_glove = Embedding(len(vocab), wordEmbeddingsGlove.shape[1], \n",
    "                                                 input_length = train_data.shape[1],\n",
    "                                                 weights = [wordEmbeddingsGlove], \n",
    "                                                 trainable = trainable) (main_input)\n",
    "    word_embeddings_pretrained_word2vec_google = Embedding(len(vocab), wordEmbeddingsGoogle.shape[1], \n",
    "                                                           input_length = train_data.shape[1],\n",
    "                                                           weights = [wordEmbeddingsGoogle], \n",
    "                                                           trainable = trainable) (main_input)\n",
    "    \n",
    "    convolution_random_1 = Conv1D(256, 32, padding = 'same', activation = 'elu')(word_embeddings_random)\n",
    "    dropout_random_1 = Dropout(0.2)(convolution_random_1)\n",
    "    max_pooling_random_1 = MaxPooling1D(pool_size = 8)(dropout_random_1)\n",
    "    conv_features_random = Flatten() (max_pooling_random_1)\n",
    "    \n",
    "    convolution_pretrained_word2vec_local_1 = Conv1D(256, 32, padding = 'same', \n",
    "                                                     activation = 'elu')(word_embeddings_pretrained_word2vec_local)\n",
    "    dropout_pretrained_word2vec_local_1 = Dropout(0.2)(convolution_pretrained_word2vec_local_1)\n",
    "    max_pooling_pretrained_word2vec_local_1 = MaxPooling1D(pool_size = 8)(dropout_pretrained_word2vec_local_1)\n",
    "    conv_features_pretrained_word2vec_local = Flatten() (max_pooling_pretrained_word2vec_local_1)\n",
    "    \n",
    "    convolution_pretrained_glove_1 = Conv1D(256, 32, padding = 'same', \n",
    "                                            activation = 'elu')(word_embeddings_pretrained_glove)\n",
    "    dropout_pretrained_glove_1 = Dropout(0.2)(convolution_pretrained_glove_1)\n",
    "    max_pooling_pretrained_glove_1 = MaxPooling1D(pool_size = 8)(dropout_pretrained_glove_1)\n",
    "    conv_features_pretrained_glove = Flatten() (max_pooling_pretrained_glove_1)\n",
    "    \n",
    "    convolution_pretrained_word2vec_google_1 = Conv1D(256, 32, padding = 'same', \n",
    "                                                     activation = 'elu')(word_embeddings_pretrained_word2vec_google)\n",
    "    dropout_pretrained_word2vec_google_1 = Dropout(0.2)(convolution_pretrained_word2vec_google_1)\n",
    "    max_pooling_pretrained_word2vec_google_1 = MaxPooling1D(pool_size = 8)(dropout_pretrained_word2vec_google_1)\n",
    "    conv_features_pretrained_word2vec_google = Flatten() (max_pooling_pretrained_word2vec_google_1)\n",
    "    \n",
    "    conv_features = keras.layers.concatenate([conv_features_random, conv_features_pretrained_word2vec_local,\n",
    "                                             conv_features_pretrained_glove, conv_features_pretrained_word2vec_google])\n",
    "    \n",
    "    dense_output_1 = Dense(hidden_layer, activation = activation) (conv_features)\n",
    "    dropout_dense_1 = Dropout(0.5)(dense_output_1)\n",
    "    dense_output_2 = Dense(hidden_layer, activation = activation) (dropout_dense_1)\n",
    "    dropout_dense_2 = Dropout(0.5)(dense_output_2)\n",
    "    main_output = Dense(train_labels.shape[1], activation = 'softmax')(dropout_dense_2)\n",
    "    \n",
    "    model = Model(inputs = [main_input], outputs = [main_output])\n",
    " \n",
    "    model.compile(optimizer = optimizer, loss = loss, metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluateModel(test_data, test_labels):   \n",
    "    scores = model.evaluate(test_data, test_labels)\n",
    "    print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "      <td>empty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "      <td>enthusiasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Re-pinging @ghostridah14: why didn't you go to...</td>\n",
       "      <td>worry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I should be sleep, but im not! thinking about ...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Hmmm. http://www.djhero.com/ is down</td>\n",
       "      <td>worry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>@charviray Charlene my love. I miss you</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>@kelcouch I'm sorry  at least it's Friday?</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>cant fall asleep</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Choked on her retainers</td>\n",
       "      <td>worry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Ugh! I have to beat this stupid song to get to...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>@BrodyJenner if u watch the hills in london u ...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Got the news</td>\n",
       "      <td>surprise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>The storm is here and the electricity is gone</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>@annarosekerr agreed</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>So sleepy again and it's not even that late. I...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>@PerezHilton lady gaga tweeted about not being...</td>\n",
       "      <td>worry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>How are YOU convinced that I have always wante...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>@raaaaaaek oh too bad! I hope it gets better. ...</td>\n",
       "      <td>worry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Wondering why I'm awake at 7am,writing a new s...</td>\n",
       "      <td>fun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>No Topic Maps talks at the Balisage Markup Con...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>I ate Something I don't know what it is... Why...</td>\n",
       "      <td>worry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>so tired and i think i'm definitely going to g...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>On my way home n having 2 deal w underage girl...</td>\n",
       "      <td>worry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>@IsaacMascote  i'm sorry people are so rude to...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Damm servers still down  i need to hit 80 befo...</td>\n",
       "      <td>worry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Fudge.... Just BS'd that whole paper.... So ti...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>I HATE CANCER. I HATE IT I HATE IT I HATE IT.</td>\n",
       "      <td>worry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29970</th>\n",
       "      <td>@mitsougelinas oui ta soeur!!</td>\n",
       "      <td>empty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29971</th>\n",
       "      <td>@esmebella Kk, I just had 888 followers like a...</td>\n",
       "      <td>surprise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29972</th>\n",
       "      <td>Hehe nah just doing this and watching Ace of C...</td>\n",
       "      <td>worry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29973</th>\n",
       "      <td>@ykitatequila OH YEAH &amp;amp; U TOO  ALL 4 OF US...</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29974</th>\n",
       "      <td>Speaking to my new tweeples.... getting acquai...</td>\n",
       "      <td>worry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29975</th>\n",
       "      <td>listening to the best days of your life by kel...</td>\n",
       "      <td>happiness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29976</th>\n",
       "      <td>Yaay congrats Shmolan on graduating  I'm proud...</td>\n",
       "      <td>happiness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29977</th>\n",
       "      <td>just got home from Tillie's dance recital. lol...</td>\n",
       "      <td>fun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29978</th>\n",
       "      <td>spent a few hours being a camera whore with pa...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29979</th>\n",
       "      <td>Pub crawling through NE pdx</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29980</th>\n",
       "      <td>@lillogs you should totally come get me and br...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29981</th>\n",
       "      <td>I'm sooo HAPPY Demi's back on twitter!</td>\n",
       "      <td>happiness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29982</th>\n",
       "      <td>Excellent, excellent movie!  Star Trek, that is.</td>\n",
       "      <td>worry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29983</th>\n",
       "      <td>@MeganWrappe Well, guess we just make a pretty...</td>\n",
       "      <td>worry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29984</th>\n",
       "      <td>@aprilcandy70  also check out @spiritjump and ...</td>\n",
       "      <td>fun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29985</th>\n",
       "      <td>is thrilled prom went well</td>\n",
       "      <td>happiness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29986</th>\n",
       "      <td>My tweet is on cheaptweet.  You wanna vote for...</td>\n",
       "      <td>fun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29987</th>\n",
       "      <td>@ThisIsRobThomas Good night, Rob. Sleep well. ...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29988</th>\n",
       "      <td>@rahsheen alright, I'll let you slide on that one</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29989</th>\n",
       "      <td>@Brandi408 thank you!</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29990</th>\n",
       "      <td>finally get my hands on my laptop!  just ate t...</td>\n",
       "      <td>fun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29991</th>\n",
       "      <td>@twtboxdj  Thanks Mr. DJ!</td>\n",
       "      <td>happiness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29992</th>\n",
       "      <td>@Oprah Happy mother's day Oprah.  You're a mom...</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29993</th>\n",
       "      <td>Just finished watching He's Just Not That Into...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29994</th>\n",
       "      <td>Hanging with my cousin Jimmy then hopefully ha...</td>\n",
       "      <td>fun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29995</th>\n",
       "      <td>I had a great date last night...tried to find ...</td>\n",
       "      <td>happiness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>With alex</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>@fureousangel that is comedy  good luck my fri...</td>\n",
       "      <td>happiness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>stephs grad party gr8! shoved cake in her face...</td>\n",
       "      <td>fun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>@jesfive SWEEEEET - San Fran is awesome!!!!  L...</td>\n",
       "      <td>happiness</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 content   sentiment\n",
       "0      @tiffanylue i know  i was listenin to bad habi...       empty\n",
       "1      Layin n bed with a headache  ughhhh...waitin o...     sadness\n",
       "2                    Funeral ceremony...gloomy friday...     sadness\n",
       "3                   wants to hang out with friends SOON!  enthusiasm\n",
       "4      @dannycastillo We want to trade with someone w...     neutral\n",
       "5      Re-pinging @ghostridah14: why didn't you go to...       worry\n",
       "6      I should be sleep, but im not! thinking about ...     sadness\n",
       "7                   Hmmm. http://www.djhero.com/ is down       worry\n",
       "8                @charviray Charlene my love. I miss you     sadness\n",
       "9             @kelcouch I'm sorry  at least it's Friday?     sadness\n",
       "10                                      cant fall asleep     neutral\n",
       "11                               Choked on her retainers       worry\n",
       "12     Ugh! I have to beat this stupid song to get to...     sadness\n",
       "13     @BrodyJenner if u watch the hills in london u ...     sadness\n",
       "14                                          Got the news    surprise\n",
       "15         The storm is here and the electricity is gone     sadness\n",
       "16                                  @annarosekerr agreed        love\n",
       "17     So sleepy again and it's not even that late. I...     sadness\n",
       "18     @PerezHilton lady gaga tweeted about not being...       worry\n",
       "19     How are YOU convinced that I have always wante...     sadness\n",
       "20     @raaaaaaek oh too bad! I hope it gets better. ...       worry\n",
       "21     Wondering why I'm awake at 7am,writing a new s...         fun\n",
       "22     No Topic Maps talks at the Balisage Markup Con...     neutral\n",
       "23     I ate Something I don't know what it is... Why...       worry\n",
       "24     so tired and i think i'm definitely going to g...     sadness\n",
       "25     On my way home n having 2 deal w underage girl...       worry\n",
       "26     @IsaacMascote  i'm sorry people are so rude to...     sadness\n",
       "27     Damm servers still down  i need to hit 80 befo...       worry\n",
       "28     Fudge.... Just BS'd that whole paper.... So ti...     sadness\n",
       "29         I HATE CANCER. I HATE IT I HATE IT I HATE IT.       worry\n",
       "...                                                  ...         ...\n",
       "29970                      @mitsougelinas oui ta soeur!!       empty\n",
       "29971  @esmebella Kk, I just had 888 followers like a...    surprise\n",
       "29972  Hehe nah just doing this and watching Ace of C...       worry\n",
       "29973  @ykitatequila OH YEAH &amp; U TOO  ALL 4 OF US...        love\n",
       "29974  Speaking to my new tweeples.... getting acquai...       worry\n",
       "29975  listening to the best days of your life by kel...   happiness\n",
       "29976  Yaay congrats Shmolan on graduating  I'm proud...   happiness\n",
       "29977  just got home from Tillie's dance recital. lol...         fun\n",
       "29978  spent a few hours being a camera whore with pa...     neutral\n",
       "29979                        Pub crawling through NE pdx        love\n",
       "29980  @lillogs you should totally come get me and br...     neutral\n",
       "29981             I'm sooo HAPPY Demi's back on twitter!   happiness\n",
       "29982   Excellent, excellent movie!  Star Trek, that is.       worry\n",
       "29983  @MeganWrappe Well, guess we just make a pretty...       worry\n",
       "29984  @aprilcandy70  also check out @spiritjump and ...         fun\n",
       "29985                         is thrilled prom went well   happiness\n",
       "29986  My tweet is on cheaptweet.  You wanna vote for...         fun\n",
       "29987  @ThisIsRobThomas Good night, Rob. Sleep well. ...     neutral\n",
       "29988  @rahsheen alright, I'll let you slide on that one     neutral\n",
       "29989                              @Brandi408 thank you!        love\n",
       "29990  finally get my hands on my laptop!  just ate t...         fun\n",
       "29991                          @twtboxdj  Thanks Mr. DJ!   happiness\n",
       "29992  @Oprah Happy mother's day Oprah.  You're a mom...        love\n",
       "29993  Just finished watching He's Just Not That Into...     neutral\n",
       "29994  Hanging with my cousin Jimmy then hopefully ha...         fun\n",
       "29995  I had a great date last night...tried to find ...   happiness\n",
       "29996                                          With alex     sadness\n",
       "29997  @fureousangel that is comedy  good luck my fri...   happiness\n",
       "29998  stephs grad party gr8! shoved cake in her face...         fun\n",
       "29999  @jesfive SWEEEEET - San Fran is awesome!!!!  L...   happiness\n",
       "\n",
       "[30000 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"./train_data.csv\"\n",
    "df = pd.read_csv(path)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data, labels = polishDataSet(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_split = 0.20\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(data, labels, \n",
    "                                                                    test_size = test_split, \n",
    "                                                                    random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = vocabBuilder(train_data, unknown = True, min_no_of_words = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data_fit = fit_unknown_token(train_data, vocab)\n",
    "test_data_fit = fit_unknown_token(test_data, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#wordModelGoogleWord2Vec = models.KeyedVectors.load_word2vec_format('../../GoogleNews-vectors-negative300.bin', binary=True)\n",
    "#wordModelGlove = models.KeyedVectors.load_word2vec_format('',binary = True)\n",
    "wordModelLocal = word2vec(train_data, 5, 2, 128, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordEmbeddingsMatrixLocal = word_embedding_matrix_builder(wordModelLocal, 128, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data_sequence = word_to_index(train_data_fit, vocab)\n",
    "test_data_sequence = word_to_index(test_data_fit, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_size = 20\n",
    "train_data_padded = sequence.pad_sequences(train_data_sequence, maxlen = train_size)\n",
    "test_data_padded = sequence.pad_sequences(test_data_sequence, maxlen = train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = makeModel(train_data_padded, train_labels, len(vocab), pretrained = False,\n",
    "                  wordEmbeddingsGlove = wordEmbeddingsMatrixLocal, wordEmbeddingsGoogle = wordEmbeddingsMatrixLocal,\n",
    "                  wordEmbeddingsLocal = wordEmbeddingsMatrixLocal, trainable = False, size = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "main_input (InputLayer)          (None, 20)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)          (None, 20, 300)       9199800     main_input[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "embedding_8 (Embedding)          (None, 20, 128)       3925248     main_input[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "embedding_9 (Embedding)          (None, 20, 128)       3925248     main_input[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "embedding_10 (Embedding)         (None, 20, 128)       3925248     main_input[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)                (None, 20, 256)       2457856     embedding_7[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)                (None, 20, 256)       1048832     embedding_8[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)                (None, 20, 256)       1048832     embedding_9[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)                (None, 20, 256)       1048832     embedding_10[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)              (None, 20, 256)       0           conv1d_5[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)              (None, 20, 256)       0           conv1d_6[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)              (None, 20, 256)       0           conv1d_7[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)              (None, 20, 256)       0           conv1d_8[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)   (None, 2, 256)        0           dropout_5[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)   (None, 2, 256)        0           dropout_6[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)   (None, 2, 256)        0           dropout_7[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)   (None, 2, 256)        0           dropout_8[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)              (None, 512)           0           max_pooling1d_5[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)              (None, 512)           0           max_pooling1d_6[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)              (None, 512)           0           max_pooling1d_7[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)              (None, 512)           0           max_pooling1d_8[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 2048)          0           flatten_5[0][0]                  \n",
      "                                                                   flatten_6[0][0]                  \n",
      "                                                                   flatten_7[0][0]                  \n",
      "                                                                   flatten_8[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 128)           262272      concatenate_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)              (None, 128)           0           dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 128)           16512       dropout_9[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)             (None, 128)           0           dense_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 13)            1677        dropout_10[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 26,860,357\n",
      "Trainable params: 15,084,613\n",
      "Non-trainable params: 11,775,744\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "batch_size = 384\n",
    "model.fit(train_data_padded, train_labels, epochs = epochs, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.evaluateModel(test_data_padded, test_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
